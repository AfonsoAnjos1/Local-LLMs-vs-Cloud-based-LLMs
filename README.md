# ğŸ§  LLM Benchmarking Framework

A Python framework for **benchmarking multiple Large Language Models (LLMs)** across various NLP datasets and tasks â€” including MMLU, SQuAD, TruthfulQA, RACE, CNN/DailyMail, and more.  
Supports **OpenAI**, **Ollama**, and **Anthropic** model providers.

---

## ğŸš€ Features

- ğŸ“Š Evaluate multiple models across standard NLP benchmarks  
- âš™ï¸ Unified interface for OpenAI, Ollama, and Anthropic APIs  
- ğŸ§® Built-in metrics: F1, ROUGE, BLEU, Edit Distance  
- ğŸ—‚ï¸ Dataset support via Hugging Face Datasets  
- ğŸ“„ Optional PDF summarization for research papers  
- ğŸ’¾ Automatic CSV export of benchmarking results  

---

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-benchmarking.git
cd llm-benchmarking

# Create virtual environment (optional)
python -m venv venv
source venv/bin/activate  # (or venv\Scripts\activate on Windows)

# Install dependencies
pip install -r requirements.txt
